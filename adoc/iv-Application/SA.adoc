
== Application

////
The Application Layer elements are typically used to model the Application Architecture that describes the structure, behavior, and interaction of the applications of the enterprise.

* *_What_* software and applications this is relevant to accomplish

Application workloads will consider the components, these will include, but not limited to SLES4SAP, SALT, TF, Repos, etc. Considerations for Availability, Performance, should be outlined here.

////

=== SUSE Linux Enterprise Server for SAP Applications

SUSE Linux Enterprise Server for SAP Applications is a product formed from a bundle of software and services.  It is targeted specifically at customers running SAP workloads.  At its foundation is SUSE Linux Enterprise Server and the High Availability Extension with many additional components and benefits for running SAP Applications.

=== SAP Application

In order to use the automation project, there are preliminary steps which need to be taken.  One of these is to prepare the SAP installation software.  The SAP software can be downloaded from https://launchpad.support.sap.com/#/softwarecenter.  This will need to be performed manually before starting the automated deployment.

=== Presenting the SAP Media

After downloading the required SAP software, the files must be presented via cloud storage so it is accessible from the new installed virtual machines / instances.

ifeval::[ "{cloud}" == "Azure" ]

Azure offers shared storage (Azure Files) for applications using the Server Message Block (SMB) protocol, providing a simple way to upload the SAP media and use it from the installed machines for the SAP installation.

To use Azure Storage, start by creating a storage account.

https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

When deploying on AWS, an S3 Bucket is required to store the SAP media.  Using the AWS Console:

* Create an S3 bucket.
* Create a folder within the bucket.
* Upload the SAP media to the folder within the S3 bucket.

TIP:  The example shows a bucket called _mysapmedia_, but a unique name should be used.

image::s3_bucket.png[scalewidth=80%]

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
When deploying on GCP, a Storage Bucket is required to store the SAP Media.  Using the GCP Console:

* Create a storage bucket.
* Create two folders (for the SAP HANA and SAP NetWeaver media) within the bucket.
* Upload the SAP Media to the folder within the storage bucket.

image::gcp_storage_bucket.png[scalewidth=80%]

TIP: The example shows a bucket called _sap-automation-media_, but a unique name should be used.

NOTE: For more information about how to create a {gcloud} Storage Bucket, refer to https://cloud.google.com/storage/docs/creating-buckets

endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt - NFS share
endif::[]

=== Terraform

Terraform is an open source tool created by HashiCorp for building, changing, and versioning infrastructure.

As an infrastructure provisioning tool, it is responsible for creating the servers, but also load balancers, queues, monitoring, subnet configurations, firewall settings, routing rules, SSL certificates, and many other infrastructure components.

Terraform is seen as cloud-agnostic and allows a single configuration to be used to manage multiple providers. This simplifies management and orchestration, helping operators build large-scale multi-cloud infrastructures.

It is important to note that Terraform can not simply create a landscape in another cloud with the press of a button. The cloud providers use different types of infrastructure.  For example the VMs, load balancers, and other services offered by AWS are very different to those in Azure and {gcloud}.

Terraform’s approach is that code is written specific to a (cloud) provider and will take advantage of the provider’s unique functionality.  While the code needs to be modified when used on a different cloud provider, being able to use the same language and toolset for all providers makes this effortless.

The name Terraform uses for these cloud specific modules is "provider." So, for example, the _Azure Provider_ can be used to configure infrastructure in Microsoft Azure using the Azure Resource Manager API's.

Configuration files describe to Terraform which components need to deploy in order to support the application.  One of the first steps is to run the `terraform` command, this will generate an execution plan that describes the actions Terraform will perform to get to the planned desired state.  The plan is in the form of a list of cloud infrastructure to create, delete, and modify.  If this looks correct, the final step is to execute the plan to create the described infrastructure.

SUSE provides Terraform configuration files for AWS, Azure, {gcloud} and libvirt.

An open source version of Terraform is shipped within the Public Cloud Module of {sles4sap}

ifeval::[ "{cloud}" == "Azure" ]
In addition, Azure provides an easy-to-access, web-based command line (Cloud Shell), where Terraform is already pre-installed.

https://shell.azure.com

You will find documentation for it at
https://docs.microsoft.com/en-us/azure/cloud-shell/overview

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

In addition, AWS provides an easy-to-access, web-based command line shell where Terraform can be downloaded and installed.

https://console.aws.amazon.com/cloudshell/

endif::[]

ifeval::[ "{cloud}" == "GCP" ]

In addition, GCP provides an easy-to-access https://shell.cloud.google.com/[web-based command line shell] where Terraform is already pre-installed.

endif::[]

As {cloud} provides different types of storage suitable for supporting SAP workloads, it is important to fully understand the SAP storage requirements for {cloud}.

The suggestions from SUSE's Terraform files for the storage configurations are meant as a good starting point.

You should still analyze the storage utilization patterns during runtime of the application.  You could realize, for example, that you are not utilizing all the storage bandwidth or IOPS provided, and you might consider downsizing storage.  Alternatively, you could see that your workload needs more storage throughput than suggested; in which case, you might choose to change the capacity, IOPS, or throughput to optimize the configuration.  {cloud} offers different storage types to allow you to find and select the right storage to support your SAP workload and meet your capacity, latency, throughput, IOPS, and cost needs.

=== SALT

SaltStack’s configuration management system lets you define the applications, files, and other settings required on a specific system. The running system is continuously evaluated against the defined configuration, and changes are made as necessary.

 * Salt works with "States" which express the required state a host should be in, using small, easy to read, easy to understand configuration files.
 * The automation is written as "formulas" which are a collection of pre-written Salt States and Salt Pillar files.
 * The Pillar files are the variables and data used to build the system.

SLES-for-SAP Applications ships with all the Salt tools as part of the distribution and are available to use as needed.

Salt formulas can be applied in two ways:

Salt Master with Salt Client:: All steps are executed on the Salt Master machine which sends instructions to the client to perform the required configuration actions.

Salt Client (Minion) only:: All steps in must be executed on each individual Salt client where the formulas need to be executed.  This is the approach used by the SUSE Automation framework as it removes the need for a central master system.


==== Overview of the available formulas which are used within the SUSE Automation framework.

===== Netweaver

The Netweaver formula for bootstrapping and managing the SAP Netweaver platform takes care of:

 * Extracting the required SAP files for SAP Media (.tar,.sar,.exe)
 * Setting up:
 ** ASCS instance
 ** ERS instance
 ** PAS instance
 ** AAS instance
 ** Database instance (currently only HANA)

Besides that, the formula sets up all of the prerequisites as:

 * Hostnames
 * Virtual addresses
 * NFS mounts
 * Shared disks
 * SWAP partition space

The Salt formula follows the best practices defined in the official SUSE documentation http://documentation.suse.com/sbp

===== HANA

The HANA bootstrap formula takes care of the following:

* Extract the required SAP files for SAP Media (.tar,.sar,.exe)
* Install SAP HANA.
* Apply "saptune" for HANA to configure and tune the OS for HANA usage
* Configure SAP System Replication.
* Preconfigure the High Availability cluster requirements.
* Configure the SAP HANA Prometheus exporter


===== HA

The HA bootstrap formula takes care of creating and managing a high availability cluster:

 * Create and configure the High Availability cluster, pacemaker, corosync, Fencing and SAP resource agents.
 * Adjustments for the {cloud} Infrastructure

ifeval::[ "{cloud}" == "Azure" ]
 * SBD for fencing
 * Handle Netweaver, HANA and DRBD
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
 * EC2 fencing
 * Adjustments for the AWS Infrastructure
 * Handle Netweaver, HANA
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
 * GCE fencing and SAP resource agents.
 * Adjustments for the {gcloud}Infrastructure
 * Handle Netweaver, HANA and DRBD
// Ab: does gce use drbd?
endif::[]

The formula provides the capability to create and configure a multi node HA cluster. Here are some of the features:

* Initialize a cluster
* Join a node to an existing cluster
* Remove a node from an existing cluster
* Configure the pre-requirements (install required packages, configure ntp/chrony, create ssh-keys, etc)
* Auto detect if the cluster is running in a cloud provider (Azure, AWS, or GCP)
* Configure fencing (agent or SBD)
* Configure Corosync
* Configure the resource agents
* Install and configure the monitoring _ha_cluster_exporter_

// SM: Q: this should be cloud specific;
// PS: A: we describe the formulas here - and there dependent services
//        second, the concept of sbd can be used at any cloud e.g. in azure we have more than one option, e.g. sbd+iscsi, sbd+rawdisk, agent
Depending on the fencing requirements it may need an iSCSI server to provide a raw shared disk for the fencing with SBD, where we use the iscsi-formula from SaltStack.

====== Other dependent services

HA NFS Service::
To build a HA NFS Service if there is none available, we can create one with help of 3 Linux services and the following

 * DRBD bootstrap formula
 * HA bootstrap formula
 * NFS formula from SaltStack to install and configure nfs server and client

iSCSI Service::
The iSCSI-formula from SaltStack is able to deploy iSNS, iSCSI initiator, and iSCSI target packages, manage configuration files and then starts the associated iSCSI services.

=== Monitoring
SUSE continually try to improve user experience. One of the developments is how to provide a modern solution to monitor the several High Availability clusters that manage SAP HANA and SAP Netweaver. The Monitoring components use the Prometheus toolkit and the Grafana project to visualize the data. In order to be able to monitor the clusters on either HANA or Netweaver, SUSE has written Prometheus exporters which ship as part of SLES for SAP.

==== SAP HANA Database Exporter
The exporter provide metrics from more than one database or tenant. It provides:

 * Memory metrics
 * CPU metrics
 * Disk usage metrics
 * I/O metrics
 * Network metrics
 * Top queries consuming time and memory

==== High Availability Cluster Exporter
Enables monitoring of Pacemaker, Corosync, SBD, DRBD and other components of High Availability clusters. This provides the ability to easily monitor cluster status and health.

 * Pacemaker cluster summary, nodes, and resource status
 * Corosync ring errors and quorum votes. Currently, only Corosync version 2 is supported.
 * Health status of SBD devices.
 * DRBD resources and connections status. Currently, only DRBD version 9 is supported.

==== SAP Host Exporter
Enables the monitoring of SAP Netweaver, SAP HANA, and other applications showing:

 * SAP start service process list
 * SAP enqueue server metrics
 * SAP application server dispatcher metrics
 * SAP internal alerts

TIP: The gathered metrics are the data that can be obtained by running the `sapcontrol` command.
