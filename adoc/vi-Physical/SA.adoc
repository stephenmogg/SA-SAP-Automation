== Physical

////
The physical elements are included as an extension to the Technology Layer for modeling the physical world. Could here be Networking, Landscape considerations

* *_Where_* the resulting solution may physically or virtually reside
////

The SAP automation consists of several building blocks, this section is in two parts, infrastructure deployment with Terraform and configuration management with Salt.

[NOTE]
====
As the project is under active development to make it better and simpler to use, this document focuses on the project version
ifeval::[ "{cloud}" == "GCP" ]
{proj_gcp_ver}
endif::[]
ifeval::[ "{cloud}" == "Azure" ]
{proj_ver}
endif::[]
ifeval::[ "{cloud}" == "AWS" ]
{proj_ver}
endif::[]
of the Terraform part and v6 of the rpm packages for Salt formulas.
The new versions could have more features or slightly changed files as shown here, but the general guidelines should still be applicable.
====


=== Prerequisites

First, make sure that all prerequisites are met:

ifeval::[ "{cloud}" == "Azure" ]

. Have an Azure account
. Install the Azure command line tool _az_
. Install _terraform_ (v12) (it comes with SLES within the public cloud module)
. Download the SAP HANA install media
. Create an Azure File Share
. Copy or write down the the name of the storage account and the storage key, which is similar to a password
. Copy the SAP HANA install media to the Azure fileshare
. Extract the HANA install media (if required)

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
. Have an AWS account, either the 'root' account, or one with enough IAM rights to run the project
. Install the AWSCLI command line tool _aws_
. Install _terraform_ (v12) (it comes with SLES within the public cloud module)
. Download the SAP HANA install media
. Create an S3 Bucket
. Copy the SAP HANA install media to a folder in the S3 Bucket
. Extract the HANA install media (optional)
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
. Have a Google Cloud Platform account
//. Install the Google Cloud SDK command line tool _aws_
. Create a Service Account with the 'Owner' role or the enough IAM rights to run the automation project
. Create a Service Account Key and save it in the machine that will be used to initiate the environment
. Install _terraform_ (v12) (it comes with SLES within the public cloud module)
. Download the SAP HANA install media from SAP
. Create a Google Cloud Storage Bucket 
//. Copy or write down the name of the Google Cloud Storage Bucket
. Copy the SAP HANA install media to a folder in the created Google Cloud Storage Bucket
. Extract the HANA install media (optional)
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt - NFS share
endif::[]

SUSE recommends to use the following directory structure:
//fixme check directory structure

----
version
  ├SWPM
  │  ├SWPMxxxxx
  │  └SAPCAR_xxxx
  │
  ├EXPORT
  │  ├xxxxEXPORT_1.zip
  │  ├xxxxEXPORT_2.zip
  │  └...
  ├DBCLIENT
  │  └IMDB_xxxx.SAR
  │
  ├BASKET
  │  ├SAPHOSTAGENTxxxx.SAR
  │  ├igshelper_xxxxxx.sar
  │  ├igsexe_xxxxx.sar
  │  ├SAPEXEDB_xxx.SAR
  │  └SAPEXE_xxx.SAR
  │
  └HANA
     ├xxxxxxxx_part1.exe
     ├xxxxxxxx_part2.rar
     ├xxxxxxxx_part3.rar
     └xxxxxxxx_part4.rar

BASKET   : contains SAP kernel, patch + more, like the hostagent.
DBCLIENT : contains the package corresponding to DB CLIENT, e.g., HANA
EXPORT   : contains the package corresponding to EXPORT files
SWPM     : contain the corresponding files of SAPCAR and of the SWPM
HANA     : contain the full HANA media
----

=== Get the project

The project is hosted on a public GitHub site where it can be downloaded to your local machine.
https://github.com/SUSE/ha-sap-terraform-deployments

The project has the following directory structure:

----
├── aws
├── azure
├── doc
├── gcp
├── generic_modules
├── libvirt
├── LICENSE
├── pillar
├── pillar_examples
├── README.md
└── salt
----

The directories with the names of the _cloud provider_ (aws, azure, gcp, libvirt) are the Terraform templates for the relevant provider.

The _doc_ directory has some brief but important documents for certain parts of the solution.

The directory _generic_modules_ provides modules which are used by all cloud vendor templates.  That includes common variables, locally executed functions within the building block, dependent actions on destroy, and the functions to start the SALT execution on the module building blocks.

The other directories _pillar_, _pillar_examples_, and _salt_ contain part of the Salt configuration management.

=== Terraform Building Blocks

Terraform relies on 'providers' to interact with remote cloud frameworks.

Providers are plugins and released independently from Terraform itself, this means that each provider has its own series of version numbers.

Each Terraform module must declare which providers it requires, so that Terraform can install and use them.

Switching into a _cloud provider_ directory shows one directory _modules_ and several _.tf_ files which together build the Terraform template.

When creating Terraform configurations, best practice is to separate out parts of the configuration into individual .tf files. This provides better organization and readability.
----
├── infrastructure.tf
├── main.tf
├── modules
├── outputs.tf
├── README.md
├── terraform.tfvars
├── terraform.tfvars.example
└── variables.tf
----

The _infrastructure.tf_ file:: provides the cloud specific setup with the relevant provider module of Terraform and defines all the needed cloud specific entities.

The _main.tf_ file:: provides all the values for the variables needed for the modules. It is the main entry point for Terraform.

The _modules_ directory:: provides more subdirectories which are the nested child modules that represent the technical building blocks in the project.

The _output.tf_ file:: provides the values returned from the modules; i.e., to be used or displayed.

_terraform.tfvars_:: is a variable definitions file which will gets automatically consumed.  This is used instead of providing values manually. It is the main configuration file and should be the only Terraform file which requires modification.

_terraform.tfvars.example_:: is an example configuration file with many pre-filled values to set up the solution. *It can be used as a starting point for your own file.*

_variables.tf_:: provides all input variables, including a short description, the type of the variable and a default value which can be overwritten with the terraform.tfvars file.
Please have a deep look at all variable and the comments for it, to get aware whats is possible.
+
E.g., the variable provisioner is like a switch to run either the Salt or Terraform portion only


A _module_:: is a container for multiple resources that are used together. Modules can be used to create lightweight abstractions, so that infrastructure can be described in terms of its architecture, rather than directly in terms of physical objects.
+
Modules are used as part of the technical building blocks; e.g., a HANA node.
+
The module directory consists of _main.tf_, _variables.tf_, and _outputs.tf_.
+
These are the recommended filenames for a minimal module, even if they are empty. _main.tf_ is the primary entrypoint for defining the infrastructure building block.

There is one additional file, _salt_provisioner.tf_, which is responsible for handing over the needed values to the Salt building blocks. This is achieved by using a special Terraform resource called _null_provider_, which remotely runs the Salt pillar to configure the instances and execute the application installation for the building block.

=== Simple Install

SUSE provides example Terraform template and Salt pillar files to provide an easy way to perform an initial simple deployment.

. Open a browser and goto https://github.com/SUSE/ha-sap-terraform-deployments
. Click on _tags_
. Click on _6.0.0_
+
What is new and what has changed can be seen from this page.  If older versions of the project are used, be sure to carefully review and understand the differences.
+
The _Usage_ section provides you with a link to an OpenBuildServer (OBS) repository where the RPM packages of the building blocks discussed above are stored. Each project version has a unique repository.
+
The value/link to the repository will need to be included within the Terraform variables (teraform.tfvars) file. So copy the line as described.

. Next go to _Assets_ and download the _Source code_ as .zip or .tar.gz
. Extract it into a folder on the local computer
. Go to this folder and into the subfolder for the cloud provider
. Copy the file _terraform.tfvars.example_ to _terraform.tfvars_
    There are many key-value variable pairs, some enabled and some disabled with a _=_ or a _#_ in front.
    In order to perform a simple deployment, only update the parameters as listed below.

ifeval::[ "{cloud}" == "Azure" ]

. Change the region in which to deploy the solution, change _az_region = "westeurope"_ to the Azure region required.

. To make it easier to start, change all 4 image types to pay-as-you-go (PAYG).  To do so, replace all _offer_ settings with "sles-sap-15-sp2" and _sku_ with gen2.
+
Do this for hana, iscsi, monitoring, drbd.
+
E.g., replace

    hana_public_offer     = "SLES-SAP-BYOS"
    hana_public_sku       = "12-sp4"
+
with

    hana_public_offer = "sles-sap-15-sp2"
    hana_public_sku   = "gen2"
+
This will make use of the on-demand images, which have all needed SUSE repositories attached automatically.
+
Next, set the name of the _admin_user_ to the name you want to use.

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

Change the region in which to deploy the solution.

----
aws_region = "eu-central-1"
----

To make it easier to start, change the images types for the SAP instances to pay-as-you-go (PAYG).  In order to do this, set the hana os image and owner accordingly.  The instances sizes/types can should also be set depending on your requirements.

----
# Instance type to use for the hana cluster nodes
hana_instancetype = "r3.8xlarge"

hana_os_image = "suse-sles-sap-15-sp2"
hana_os_owner = "679593333241"
----

To automatically deploy PAYG instances from the AWS Marketplace, be sure to 'Subscribe' to the offering.

A link for SLES for SAP 15 SP2 can be found here.
https://aws.amazon.com/marketplace/server/procurement?productId=e9701ac9-43ee-4dda-b944-17c6c231c8db

If a different version of SLES for SAP is required, subscribe to the relevant version on the marketplace.

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

. The next step is to provide ssh keys to access the machines that will be deployed.
+
SUSE recommends creating new sshkeys for the deployment. Both public and private keys will need to be provided, as they are copied to the cluster nodes during the deployment.
+
Change the two location variables and point them to your files.
+

ifeval::[ "{cloud}" == "Azure" ]

. As the SAP Install Media is needed for the automatic deployment of HANA, an Azure storage account needs to be created.  The SAP HANA media will need to be copied to this storage location. If the SAP media is already extracted this will save time during the deployment.
+
Next, provide the name, key, and path to this storage account, change:

    storage_account_name
    storage_account_key
    hana_inst_master
+
The inst_master variable should point to the directory where you have the extracted the hana install files.
There are more possibilities, but, for simplicity, have everything already extracted on your share.
+
Disable the other hana variables by adding a '#' in front of them:

   #hana_archive_file = "IMDB_SERVER.SAR"
   #hana_sapcar_exe = "SAPCAR"
   #hana_extract_dir = "/sapmedia/HDBSERVER"

. Additional ssh keys are needed for the cluster communications, so please save your changes and run the following commands from the azure directory:
+
[subs="attributes,quotes"]
----
   mkdir -p ../salt/hana_node/files/sshkeys
   ssh-keygen -t rsa -N '' -f ../salt/hana_node/files/sshkeys/cluster.id_rsa
----

. Open the tfvars file again to make final changes.
+
To create a HANA Systemreplication HA automation, uncomment:

    #hana_ha_enabled = true
+
by removing the _#_.
+
Next, we need to enable a few other services. Uncomment:

    #hana_cluster_sbd_enabled = true
+
by removing the _#_.

. Now we need to point to where the right packages for the v6 could be found. Copy the variable from step 1; e.g.,
+
[subs="attributes,quotes"]
----
    ha_sap_deployment_repo = "https://download.opensuse.org/repositories/network:ha-clustering:sap-deployments:v6"
----

. If you want the additional monitoring be deployed, simply uncomment:

    #monitoring_enabled = true

. As the last step, we enable a simplification parameter which tries to determine a few settings automatically. So scroll down to the end and uncomment

    #pre_deployment = true

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

This setting provides credentials to Terraform in order to deploy infrastructure on the AWS cloud.

----
aws_credentials = "~/.aws/credentials"
----

Modify the following to point to the SAP Media that was uploaded to the S3 Bucket earlier.  The credentials provided above require permissions to read from this S3 bucket.

----
hana_inst_master = "s3://mysapmedia/SAPHANA"

hana_archive_file = "51052481_part1.exe"
----

To keep the cluster architecture simple and to provide additional packages needed to deploy, set the following.

----
hana_cluster_sbd_enabled = false

# Repository url used to install HA/SAP deployment packages"
ha_sap_deployment_repo = "https://download.opensuse.org/repositories/network:ha-clustering:sap-deployments:v6"
pre_deployment = true
----

Finally, ensure the following lines are commented *out* using a #:

----
# hana_disk_device = "/dev/xvdd"
# aws_access_key_id = my-access-key-id
# aws_secret_access_key = my-secret-access-key
----

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]


We are nearly done, so take a moment to save your changes before proceeding.

. Go one directory up, change to the _pillar_example_ directory, and then change to the _automatic_ directory.  Here you can see 3 additional subdirectories. They provide the configuration variables for the relevant services. This _automatic_ folder will work for all cloud providers we support today, which is why it is more complex.

. For a simple deployment, which uses only HANA, please switch to the _hana_ directory and open the file _hana_sls_.

. Change the PRIMARY_SITE_NAME to the desired value, along with value for the SECONDARY_SITE_NAME.
It is possible to change other settings (e.g., passwords), but, for a simple test, do not modify these values.

. Save any changes to the file and and go back to the main directory.


We are now ready to run Terraform.

[subs="attributes,quotes"]
----
    az login
    terraform init
    terraform workspace new yourprojectname
    terraform plan
    terraform apply
----

ifeval::[ "{cloud}" == "Azure" ]

If all goes well, after ~40 minutes (depending on the speed of the instances) you will have an installed and running HANA System Replication Cluster.

As a jumphost with a public ip address is created as part of the deployment, it is possible to log in to any virtual machine as part of the deployment from your machine with

[subs="attributes,quotes"]
----
  ssh -J adminuser@jumphost adminuser@targethost
----

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

If all goes well, after ~30-40 Minutes a fully configured HANA System Replication Cluster will have been deployed.

The instances are currently provisioned with a public IP address as part of the deployment, you can ssh to them directly using the ec2-user.

----
    ssh ec2-user@public_ip_of_hana_node
----

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]


==== Terraform file details

All files in the Terraform directory using the .tf file format will be automatically loaded during operations.

The _infrastructure.tf_ provides the _data sources_ for the network setup. This is computed in other terraform files and some _local_ variables, used for mainly for the autogeneration of the network.

ifeval::[ "{cloud}" == "Azure" ]
In addition, it provides the _resources_ for the network setup with the virtual network, the subnet and routing, the resourcegroup to be used, a storage account, all the network security groups (nsg), and definition of the jumphost.
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
In addition, it provides the _resources_ for the network setup, including VPCs, Security Groups, Public IP etc.
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

The _main.tf_ file is the main file and calls child modules, which consist of the various building blocks and the required input and output variables defined by the child modules.
In addition, it provides the calculation for the autogenerated IP addresses.

There is the (default) possibility to autogenerate network addresses for all nodes.
For this, it is important to remove or comment out all the variables related to the IP addresses (more information in variables.tf). With this approach, all the addresses will be retrieved based on the provided virtual network address range (vnet_address_range).

ifeval::[ "{cloud}" == "Azure" ]

.Autogenerated addresses example based on 10.74.0.0/16 vnet address range and 10.74.0.0/24 subnet address range
[with="70%",options="header"]
|==========================
| Name         | Terraform variable | IP Address | Comment
| iSCSI server | iscsi_srv_ip       | 10.74.0.4  | needed for SBD device in HA configuration
| Monitoring   | monitoring_srv_ip  | 10.74.0.5  | if monitoring is enabled
| HANA IP's    | hana_ips           | 10.74.0.10, 10.74.0.11 | second only used in HA
| Hana cluster virtual IP | hana_cluster_vip | 10.74.0.12 | Only used if HA is enabled in HANA
| Hana cluster virtual IP secondary | hana_cluster_vip_secondary | 10.74.0.13 | Only used if the Active/Active HA setup is enabled
| DRBD IP's    | drbd_ips | 10.74.0.20, 10.74.0.21 | needed if HA NFS service for NW is used
| DRBD cluster vIP | drbd_cluster_vip | 10.74.0.22 |needed if HA NFS service for NW is used
| Netweaver IP's | netweaver_ips | 10.74.0.30, 10.74.0.31, 10.74.0.32, 10.74.0.33 | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines
| Netweaver virtual IP's | netweaver_virtual_ips | 10.74.0.34, 10.74.0.35, 10.74.0.36, 192.168.135.37 | The 1st virtual address will be the next in the sequence of the regular Netweaver addresses
|==========================

endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS

Within AWS, the Availability Zones (AZ) of a VPC get used for the HA scenario.
Each of the AZs has its own network and, therefore, each of the machines in a cluster is in a different subnet. The floating virtual IP address is created with help of a special resource agent, which changes the routing table entry of a virtual router for VPC so the adress is outside of the VPC and AZs

Example based on `10.0.0.0/16` address range (VPC address range) and `192.168.1.0/24` as `virtual_address_range` (the default value).

[with="80%",options="header"]
|==========================
| Name | Substituted variable | Addresses | Comments
| Iscsi server | `iscsi_srv_ip` | `10.0.0.4` |
| Monitoring | `monitoring_srv_ip` | `10.0.0.5` |
| Hana ips | `hana_ips` | `10.0.1.10`, `10.0.2.11` |
| Hana cluster vip | `hana_cluster_vip` | `192.168.1.10` | Only used if HA is enabled in HANA
| Hana cluster vip secondary | `hana_cluster_vip_secondary` | `192.168.1.11` | Only used if the Active/Active setup is used
| DRBD ips | `drbd_ips` | `10.0.5.20`, `10.0.6.21` |
| DRBD cluster vip | `drbd_cluster_vip` | `192.168.1.20` |
| Netweaver ips | `netweaver_ips` | `10.0.3.30`, `10.0.4.31`, `10.0.3.32`, `10.0.4.33` | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines
| Netweaver virtual ips | `netweaver_virtual_ips` | `192.168.1.30`, `192.168.1.31`, `192.168.1.32`, `192.168.1.33` | The last number of the address will match with the regular address
|==========================
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP

Example based on `10.0.0.0/24` VPC address range. The virtual addresses must be outside of the VPC address range.

[with="70%",options="header"]
|==========================
| Name | Substituted variable | Addresses | Comments
| Iscsi server | `iscsi_srv_ip` | `10.0.0.4` |
| Monitoring | `monitoring_srv_ip` | `10.0.0.5` |
| Hana ips | `hana_ips` | `10.0.0.10`, `10.0.0.11` |
| Hana cluster vip | `hana_cluster_vip` | `10.0.2.12` | Only used if HA is enabled in HANA
| Hana cluster vip secondary | `hana_cluster_vip_secondary` | `10.0.1.13` | Only used if the Active/Active setup is used
| DRBD ips | `drbd_ips` | `10.0.0.20`, `10.0.0.21` |
| DRBD cluster vip | `drbd_cluster_vip` | `10.0.1.22` |
| Netweaver ips | `netweaver_ips` | `10.0.0.30`, `10.0.0.31`, `10.0.0.32`, `10.0.0.33` | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines
| Netweaver virtual ips | `netweaver_virtual_ips` | `10.0.1.34`, `10.0.1.35`, `10.0.1.36`, `10.0.1.37` | The 1st virtual address will be the next in the sequence of the regular Netweaver addresses
|==========================
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt

Example based on `192.168.135.0/24` address range.

[with="70%",options="header"]
|==========================
| Name | Substituted variable | Addresses | Comments
| Iscsi server | `iscsi_srv_ip` | `192.168.135.4` |
| Monitoring | `monitoring_srv_ip` | `192.168.135.5` |
| Hana ips | `hana_ips` | `192.168.135.10`, `192.168.135.11` |
| Hana cluster vip | `hana_cluster_vip` | `192.168.135.12` | Only used if HA is enabled in HANA
| Hana cluster vip secondary | `hana_cluster_vip_secondary` | `192.168.135.13` | Only used if the Active/Active setup is used
| DRBD ips | `drbd_ips` | `192.168.135.20`, `192.168.135.21` |
| DRBD cluster vip | `drbd_cluster_vip` | `192.168.135.22` |
| Netweaver ips | `netweaver_ips` | `192.168.135.30`, `192.168.135.31`, `192.168.135.32`, `192.168.135.33` | Addresses for the ASCS, ERS, PAS and AAS. The sequence will continue if there are more AAS machines
| Netweaver virtual ips | `netweaver_virtual_ips` | `192.168.135.34`, `192.168.135.35`, `192.168.135.36`, `192.168.135.37` | The 1st virtual address will be the next in the sequence of the regular Netweaver addresses
|==========================
endif::[]

In order to reuse existing network resources (virtual network and subnets), configure the _terraform.tfvars_ file and adjust the relevant variables.

An example of how to use them is available at _terraform.tfvars.example_.

[IMPORTANT]
====
If specifying the IP addresses manually, make sure these are valid IP addresses. They should not be currently in use by existing instances. In the case of shared account usage in cloud providers, it is recommended to set unique addresses with each deployment to avoid using the same addresses.
====

The _output.tf_ file is a way to expose some of the internal attributes. These act like the return values of a Terraform module to the user. It will return the IP address and node names created from the automation.

The values defined in the _variables.tf_ file are used to avoid hard-coding parameters, and it provides all required Terraform input variables and their default values within the solution instead of having them in the main.tf file.

As there are many variable values to input, these need to be defined in a variable definition file named _terraform.tfvars_. Terraform will automatically load the variable values from the variable definition file if it is named terraform.tfvars.

The _modules_ directory provides all the needed resources to create the respective building block
----
modules/
├── bastion
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── drbd_node
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── hana_node
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── iscsi_server
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── monitoring
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
├── netweaver_node
│   ├── main.tf
│   ├── outputs.tf
│   ├── salt_provisioner.tf
│   └── variables.tf
└── os_image_reference
    ├── outputs.tf
    └── variables.tf
----

The respective _salt_provisioner.tf_ file sets the *_role_* of the *node* and, with the help of a Terraform file provisioner, will pass the needed variables which were set in Terraform *as custom Salt _grains_ for the node* and starts the Salt provisioning process.

==== SAP Sizing

One of the key points to consider in an SAP deployment is sizing and applies across three key areas: compute power, storage space and I/O capacity, and network bandwidth.

If this is a greenfield deployment, please use the SAP Quick Sizer tool to calculate the SAP Application Performance Standard (SAPS) compute requirement and choose the right instance types with the closest match to the performance needed.

If you have an SAP system running that you want to extend with new functionality and/or add new users or migrate to SAP HANA, perform brownfield sizing.

Overall it is an iterative and continuous process to translate your business requirements to the correct (virtual) hardware resources.

This is a mandatory step and should not be underestimated.


ifeval::[ "{cloud}" == "Azure" ]

SUSE makes it easier to deploy the right instance sizes with the right disks types and performance, as well as the right network settings. A simplified SAP sizing has been introduced with well known T-Shirt sizes, S, M, L, and a very small Demo size.

Behind the sizes, are useful combinations to provide certain SAP performance scenarios.

Below is a simple reference of the possible performance values

* Demo
* Small  <  30.000 SAPS
* Medium <  70.000 SAPS
* Large  < 180.000 SAPS

It is possible to customize the settings within the _terraform.tfvars_ file, or provide a permanent solution in the variables file.

The Demo and Small size are designed for non-production scenarios and do not use SAP certified instance types, whereas the Medium and Large are meant for production usage and therefore use SAP certified instance types. The setups also use the correct disks and I/O behavior for production.

The SAPS values are meant for the landscape and not only for the database.

===== HANA

Given that low storage latency is critical for database systems, even for in-memory systems as SAP HANA. The critical path in storage is usually around the transaction log writes of the DB systems, but other operations like savepoints or loading data in-memory after crash recovery can be critical.

Therefore, it is mandatory to leverage Azure premium storage or Ultra disk for /hana/data and /hana/log volumes. Depending on the performance requirements, we may need to build a RAID-0 stripe-set to aggregate IOPS and throughput to meet the application scenario need.

The overall VM I/O throughput and IOPS limits need to be kept in mind when deciding on a instance type.

Actual recommendations could be found at the following URL:
https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/hana-vm-operations-storage

The maps below describe how the disks for SAP HANA will be used and created during the provisioning.

disks_type:: As HANA has high I/O requirements the disk type Premium SSD needs to be used.
disks_size:: The size of the additional disk is expressed in GB. Every size has certain IOPS caps.
caching:: The caching recommendations for Azure premium disks assume the I/O characteristics for SAP HANA, as follows:
+
* /hana/data - no caching or read caching
* /hana/log - no caching - exception for M- and Mv2-Series VMs where Azure Write Accelerator should be enabled
* /hana/shared - read caching

writeaccelerator:: Azure Write Accelerator is a functionality that is available for Azure M-Series VMs exclusively. As the name implies, the purpose of the functionality is to improve I/O latency of writes against the Azure premium storage. For SAP HANA, Write Accelerator is supposed to be used against the /hana/log volume only. Therefore, the /hana/data and /hana/log are separate volumes with Azure Write Accelerator supporting the /hana/log volume only.

Number of Disks:: The number of disks which get used, depend on the performance requirements. We join disks to a stripe set to provide more performance. At a minimum we need 4 to 5 disks.

LogicalVolumes::  We are using LVM to build stripe sets across several Azure premium disks. These stripe sizes differ between /hana/data and /hana/log. The recommendations are:
+
* 256 KB for /hana/data
* 64 KB for /hana/log

Name of the VolumeGroup:: This is the name of the volume group used.

Mount path:: This is the mount point where the volume gets mounted.


The number of elements *must match* in all of them.

_#_ (hash character):: is used to split the volume groups.
+
The number of groups split by "#" *must match* in all of the entries

_,_ (comma):: is used to define the logical volumes for each volume group.


_names_:: The names of the volume groups (e.g., datalog#shared#usrsap#backup#sapmnt).

_luns_:: The luns or disks used for each volume group. The number of luns must match with that configured in the previous disks variables (e.g., 0,1,2#3#4#5#6).

_sizes_:: The size dedicated for each logical volume and folder (e.g, 70,100#100#100#100#100).

_paths_:: Folder where each volume group will be mounted (e.g., /hana/data,/hana/log#/hana/shared#/usr/sap#/hana/backup#/sapmnt/).

The values could be set with the variables "hana_vm_size", "hana_enable_accelerated_networking," and "hana_data_disks_configuration" in the _variables.tf_ file if a change to the default (demo) is needed or, better still, in the _terraform.tfvars_ to set actual values.

===== Netweaver

NetWeaver is SAP's integrated technology platform and is not a product in itself, but it provides the required services for the SAP business applications and always needs a database.

It is the overall task of sizing to fulfil the requirements of Netweaver plus the database, and this is what is combined within the T-Shirt sizes of the solution.


Details of the solution T-Shirt sizes are provided below.


====== Demo

HANA instance size:: Standard_E4s_v3
Accelerated networking:: false

.HANA disk configuration details
[with="90%",cols="10,40"]
|==========================
|disks_type|Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS
|disks_size|128,128,128,128,128,128,128"
|caching   |None,None,None,None,None,None,None"
|writeaccelerator |false,false,false,false,false,false,false"
|luns      |0,1#2,3#4#5#6#7"
|names     |data#log#shared#usrsap#backup"
|lv_sizes  |100#100#100#100#100"
|paths     |/hana/data#/hana/log#/hana/shared#/usr/sap#/hana/backup
|==========================

.Netweaver configuration variables
[with="90%",cols="40,40"]
|==========================
|netweaver_xscs_vm_size      | Standard_D2s_v3
|netweaver_app_vm_size       | Standard_D2s_v3
|netweaver_data_disk_type    | Premium_LRS
|netweaver_data_disk_size    | 128
|netweaver_data_disk_caching | ReadWrite
|netweaver_xscs_accelerated_networking | false
|netweaver_app_accelerated_networking | false
|netweaver_app_server_count  | 2
|==========================

====== Small

HANA instance size:: Standard_E64s_v3
Accelerated networking:: true

.HANA disk configuration details
[with=90%",cols="10,40"]
|==========================
| disks_type       | Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS
| disks_size       | 512,512,512,512,64,1024
| caching          | ReadOnly,ReadOnly,ReadOnly,ReadOnly,ReadOnly,None
| writeaccelerator | false,false,false,false,false,false
| luns             | 0,1,2#3#4#5
| names            | datalog#shared#usrsap#backup
| lv_sizes         | 70,100#100#100#100
| paths            | /hana/data,/hana/log#/hana/shared#/usr/sap#/hana/backup
|==========================

.Netweaver configuration details
[with="90%",cols="40,40"]
|==========================
|netweaver_xscs_vm_size | Standard_D2s_v3
|netweaver_app_vm_size | Standard_D2s_v3
|netweaver_data_disk_type | Premium_LRS
|netweaver_data_disk_size | 128
|netweaver_data_disk_caching | ReadWrite|netweaver_xscs_accelerated_networking | false
|netweaver_app_accelerated_networking | false
|netweaver_app_server_count | 2
|==========================

====== Medium

HANA instance size:: Standard_M64s
Accelerated networking:: true

.HANA disk configuration details
[with="90%",cols="10,40"]
|==========================
| disks_type       | Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS
| disks_size       | 512,512,512,512,512,512,1024,64,1024,1024
| caching          | ReadOnly,ReadOnly,ReadOnly,ReadOnly,None,None,ReadOnly,ReadOnly,ReadOnly,ReadOnly
| writeaccelerator | false,false,false,false,false,false,false,false,false,false
| luns             | 0,1,2,3#4,5#6#7#8,9
| names            | data#log#shared#usrsap#backup
| lv_sizes         | 100#100#100#100#100
| paths            | /hana/data#/hana/log#/hana/shared#/usr/sap#/hana/backup
|==========================

.Netweaver configuration details
[with="90%",cols="40,40"]
|==========================
|netweaver_xscs_vm_size | Standard_D2s_v3
|netweaver_app_vm_size | Standard_E64s_v3
|netweaver_data_disk_type | Premium_LRS
|netweaver_data_disk_size | 128
|netweaver_data_disk_caching | ReadWrite
|netweaver_xscs_accelerated_networking | false
|netweaver_app_accelerated_networking | true
|netweaver_app_server_count | 5
|==========================

====== Large

HANA instance size:: Standard_M128s
Accelerated networking:: true

.HANA disk configuration details
[with="90%",cols="10,40"]
|==========================
| disks_type       | Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS,Premium_LRS
| disks_size       | 1024,1024,1024,512,512,1024,64,2048,2048
| caching          | ReadOnly,ReadOnly,ReadOnly,None,None,ReadOnly,ReadOnly,ReadOnly,ReadOnly
| writeaccelerator | false,false,false,true,true,false,false,false,false
| luns             | 0,1,2#3,4#5#6#7,8
| names            | data#log#shared#usrsap#backup
| lv_sizes         | 100#100#100#100#100
| paths            | /hana/data#/hana/log#/hana/shared#/usr/sap#/hana/backup
|==========================

.Netweaver configuration details
[with="90%",cols="40,40"]
|==========================
|netweaver_xscs_vm_size | Standard_D2s_v3
|netweaver_app_vm_size | Standard_E64s_v3
|netweaver_data_disk_type | Premium_LRS
|netweaver_data_disk_size | 128
|netweaver_data_disk_caching | ReadWrite
|netweaver_xscs_accelerated_networking | false
|netweaver_app_accelerated_networking | true
|netweaver_app_server_count | 10
|==========================

endif::[]

ifeval::[ "{cloud}" == "AWS" ]

Currently, there is no sizing built into the SUSE Automation tooling.
The instance size will determine the capability of the deployment, and the disk size is fixed at 60GB (single EBS volume).
These can be modified by editing the _main.tf_ file in the ~/aws/modules/hana_node/ directory.

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
Libvirt
endif::[]

=== Salt Building Blocks

Resources are the most important elements in Terraform. There is another resource type used as last a step from the Terraform process, the _Provisioner_ resource.

It can be used to model specific actions on a remote machine in order to prepare them for other services.

The Terraform _file provisioner_ is used to copy directories _MAIN_/salt and _MAIN_/pillar from the machine executing Terraform to the newly created nodes.

Finally, the Terraform _remote-exec provisioner_ is used to call the script, _provision.sh_, on the remote node to run the Salt provisioning steps. It comes from the Terraform module _MAIN/generic_modules/salt_provisioner/main.tf_.

*From this point on, all work is performed on the respective node itself.*

==== Our Architecture for the Salt building blocks

//fixme - image our salt module arch.
image::shap_deployment_small.png[role="right",scaledwidth="40%"]

shaptools:: low level python wrapper (API) around SAP utilities and commands

Execution module:: provides the methods in the lower layer (shaptools) to Salt

State:: combination of execution modules and other parts with logic to define a specific configuration

Formula:: group of states that give a context for building blocks; e.g., HANA



The provisioning workflow of the SAP building blocks consist of different steps:

. Bootstrap Salt installation and configuration.

. Perform OS setup operations; register to SCC, if needed; update the packages; etc. by executing the states within _/srv/salt/os_setup_.

. Perform predeployment operations by execution of the _/srv/salt/top.sls_ states. It updates hosts and hostnames, installs the formula packages, etc.

. Perform deployment operations depending on the overall configuration settings; e.g., install SAP applications, configure and setup HA with the salt formulas.


==== Salt Overview
The SAP building blocks are created with help of Salt formulas after provisioning the virtual machines with Terraform. The formulas are shipped as RPM packages with {sles4sap}.

The Salt formulas can be used with two different approaches: Salt master/minion or only Salt minion execution.

In this automation solution, we use the Salt minion option. The steps in the formulas must be executed in all of the minions and are performed through a SSH connection.

The core of the Salt State system is the SLS, or **S**a**L**t **S**tate file. The SLS is a representation of the state in which a system is expected to be, and is set up to contain this data in a simple format.

There are 3 types of Salt files used

pillar files:: the _configuration_ parameters where the data gets imported with help of jinja (map.jinja) and Salt['pillar.get']

state files:: the _execution_ definition in /srv/salt

grains files:: _environment_ parameters from the node itself and for handing over variables from Terraform; e.g., /etc/salt/grains

In Salt, the file which contains a mapping between groups of machines on a network and the configuration roles that should be applied to them is called a top file.

Top files are named _top.sls_ by default, and they are so named because they always exist in the "top" of a directory hierarchy, called a state tree, that contains state files.


===== Salt pillar

Similar to the state tree, the pillar is comprised of .sls files and has a top file too. The default location is /srv/pillar.

The pillar files define custom variables and data for a system.

When Salt pillar data is refreshed, each Salt minion is matched against the targets listed in the _top.sls_ file. When a Salt minion matches a target, it receives all of the Salt pillar SLS files defined in the list underneath that target.

*Directory structure for pillars*
[subs="attributes,quotes"]
----
/srv
├── pillar
│   ├── *top.sls*
│   ├── drbd
│   │   ├── cluster.sls
│   │   └── drbd.sls
│   ├── hana
│   │   ├── cluster.sls
│   │   └── hana.sls
│   ├── iscsi_srv.sls
│   └── netweaver
│       ├── cluster.sls
│       └── netweaver.sls
├── salt
----

The _top.sls_ pillar file describes the needed data for the respective role of the node.

*State top.sls file*
[subs="attributes,quotes"]
----
base:
  'role:iscsi_srv':
    - match: grain
    - iscsi_srv

  'role:hana_node':
    - match: grain
    - hana.hana

  'G@role:hana_node and G@ha_enabled:true':
    - match: compound
    - hana.cluster

  'role:drbd_node':
    - match: grain
    - drbd.drbd
    - drbd.cluster

  'role:netweaver_node':
    - match: grain
    - netweaver.netweaver

  'G@role:netweaver_node and G@ha_enabled:true and P@hostname:.*(01|02)':
    - match: compound
    - netweaver.cluster
----

To run an initial deployment without specific customization, use pillar files stored in the _MAIN/pillar_example/automatic_ folder, as these files are customized with parameters coming from Terraform execution. The pillar files stored there are able to deploy a basic functional set of clusters in all of the available cloud providers.

To adapt the deployment to your scenario, provide your own pillar data files.  There are some basic examples within the directory _MAIN/pillar_example_.
As the pillar files provide data for the Salt formulas, all of the possible pillar options can be found in each formula project.

// fixme - which versions are in sles4sap?
//- this need to be in a document instead of the all the different github projects
//- https://github.com/SUSE/saphanabootstrap-formula (HANA configuration)
//- https://github.com/SUSE/habootstrap-formula (HA cluster configuration)
//- https://github.com/SUSE/drbd-formula (DRBD configuration)
//- https://github.com/SUSE/sapnwbootstrap-formula (NETWEAVER or S4/HANA configuration)

[IMPORTANT]
====
Pillar files are expected to contain private data, such as passwords, required for automated installation or other operations. Therefore, such pillar data need to be stored in an encrypted state, which can be decrypted during pillar compilation.

SaltStack GPG renderer provides a secure encryption/decryption of pillar data. The configuration of GPG keys and procedure for pillar encryption are described in the Saltstack documentation guide:

. https://docs.saltstack.com/en/latest/topics/pillar/#pillar-encryption[SaltStack pillar encryption]

. https://docs.saltstack.com/en/latest/ref/renderers/all/salt.renderers.gpg.html[SaltStack GPG RENDERERS]

*Encryption is not included in this automation solution.  You are strongly advised to take appropriate security precautions.*
====


===== Salt states

_Salt state_ files are organized into a directory tree, called the Salt state tree, in the /srv/salt/ directory.

*Directory structure for Salt state files*
[subs="attributes,quotes"]
----
/srv
├── pillar
....
├── salt
│   ├── cluster_node
│   │   ├──
│   ├── default
│   │   ├──
│   ├── drbd_node
│   │   ├──
│   ├── hana_node
│   │   ├──
│   ├── iscsi_srv
│   │   ├──
│   ├── _modules
│   │   ├──
│   ├── monitoring_srv
│   │   ├──
│   ├── netweaver_node
│   │   ├──
│   ├── os_setup
│   │   ├──
│   ├── provision.sh
│   ├── qa_mode
│   │   ├──
│   ├── sshkeys
│   │   ├──
│   ├── _states
│   │   ├──
│   └── **top.sls**
----

Within this directory structure, all needed steps that depend on the _role_ of the node can be seen.

The _top.sls_ file describes two environments for the nodes, _pre-deployment_ and _base_ which reflect steps 3 and 4 of the workflow above.

//For each role of the nodes there more detailed files responsible.//

The Pre-deployment environment is needed, as formulas can not be installed and used directly in the same execution.

*State top.sls file*
[subs="attributes,quotes"]
----
predeployment:
  'role:hana_node':
    - match: grain
    - default
    - cluster_node
    - hana_node

  'role:netweaver_node':
    - match: grain
    - default
    - cluster_node
    - netweaver_node

  'role:drbd_node':
    - match: grain
    - default
    - cluster_node
    - drbd_node

  'role:iscsi_srv':
    - match: grain
    - iscsi_srv

  'role:monitoring_srv':
    - match: grain
    - default
    - monitoring_srv

base:
  'role:hana_node':
    - match: grain
    - hana

  'G@role:hana_node and G@ha_enabled:true':
    - match: compound
    - cluster

  'role:drbd_node':
    - match: grain
    - drbd
    - cluster

  'role:netweaver_node':
    - match: grain
    - netweaver

  'G@role:netweaver_node and G@ha_enabled:true and P@hostname:.*(01|02)':
    - match: compound
    - cluster
----

===== Salt grains

SaltStack comes with an interface to derive information about the underlying system. This is called the _grains_ interface, because it presents Salt with grains of information.
It collects static informations about the underlying managed system, such as the operating system, domain name, IP address, kernel, OS type, memory, and many other system properties.
The SUSE Automation project uses custom grains to match the roles and the further states.

The _role_ is a _custom grains_ defined with help of the Terraform file _salt_provisioner.tf_ for the respective building block.

[CAUTION]
====
If using the Salt formulas independently from the Terraform templates, it is important to take care of providing all required variables that would normally get set by the _salt_provisioner.tf_.
====


===== State details

If targeting a directory during a _state.apply_ or in the state Top file, Salt looks for an init.sls file in that directory and applies it.

Within the _os_setup_ directory

[subs="attributes,quotes"]
----
│   ├── os_setup
│   │   ├── init.sls
│   │   ├── ip_workaround.sls
│   │   ├── *minion_configuration.sls*
│   │   ├── packages.sls
│   │   ├── registration.sls
│   │   └── repos.sls
----

there is one interesting file, the _minion_configuration.sls_. It provides the configuration how and where Salt / the Minion looks for Salt states and Salt formulas.


Looking deeper into one of the directories, _hana-node_, there are more files.

*HANA Node state files*
[subs="attributes,quotes"]
----
│   ├── *hana_node*
│   │   ├── download_hana_inst.sls
│   │   ├── files
│   │   │   └── sshkeys
│   │   │       ├── cluster.id_rsa
│   │   │       └── cluster.id_rsa.pub
│   │   ├── hana_inst_media.sls
│   │   ├── hana_packages.sls
│   │   ├── *init.sls*
│   │   └── mount
│   │       ├── azure.sls
│   │       ├── gcp.sls
│   │       ├── *init.sls*
│   │       ├── mount.sls
│   │       ├── mount_uuid.sls
│   │       └── packages.sls
----


When targeting a directory during a _state.apply_ or in the state Top file, Salt looks for an init.sls file in that directory and applies it.
Salt executes what is in _init.sls_ in the order listed in the file. When a Salt file is named init.sls, it inherits the name of the directory path that contains it.
This formula/state can then be referenced with the name of the directory.

In our case here, it first gets the SAP HANA Media with help of _hana_ins_media_, creates the mountpoints, partitions disks for SAP HANA, and enters them into the fstab with help of the states in the _mount_ directory. Similar as before, the starting point is again the _init.sls_ file.

After all is processed within _mount_, it gets back to the file _hana_packages_. It then installs the RPM packages, _shaptools_ and _saphanabootstrap-formula_, which get shipped with {sles4sap}.

All other states files get processed in the same way as the example above.


==== Salt formula packages

Formulas are pre-written Salt states. They are as open-ended as Salt States themselves, and they can be used for tasks such as installing a package, configuring and starting a service, setting up users or permissions, and many other common tasks.
Each formula is intended to be immediately usable with the sane defaults and no additional configuration.

The formulas in the automation solution are configurable by including data in _Pillar_ files, as discussed above.
During RPM install, the files of the packages end up in the directory _/usr/share/salt-formulas/states_. This was defined as the directory where Salt searches for files in addition to /srv/salt (see os_setup state above).

.shaptools package
The directories _modules_ and _states_ come from the install of the package shaptools and provide a python wrapper as an API for sap command line tools, making it simpler to with Salt.
This package is a base dependency for most of the SUSE formula packages as it provides the needed SAP commands.

[subs="attributes,quotes"]
----
│   ├── _modules
│   │   ├── ...
│   ├── _states
│   │   ├── ...
----

===== HANA formula

The main work of preparing the node for HANA and installing HANA is performed by the _saphanabootstrap-formula_.

The structure is similar to what has been seen above for pillars and states but lives in the directory _/usr/share/salt-formulas/states/..._

[subs="attributes,quotes"]
----
states/
└── hana
    ├── defaults.yaml
    ├── enable_cost_optimized.sls
    ├── enable_primary.sls
    ├── enable_secondary.sls
    ├── exporter.sls
    ├── *init.sls*
    ├── install.sls
    ├── map.jinja
    ├── packages.sls
    ├── pre_validation.sls
    └── templates
        ├── hanadb_exporter.j2
        ├── scale_up_resources.j2
        └── srTakeover_hook.j2
----

Salt includes the Jinja2 template engine which can be used in Salt state files, Salt pillar files, and other files managed by Salt.
Salt lets you use Jinja to access minion configuration values, grains, and Salt pillar data, and to call Salt execution modules.
One of the most common uses of Jinja is to insert conditional statements into Salt pillar files.

. The formula package is installed through the HANA Node state files

. To install it manually please use zypper, as this will include the other dependent packages such as salt-shaptools and habootstrap-formula

----
 zypper install saphanabootstrap-formula
----

The Salt formula will need input data through a pillar file which is part of the main project file (in MAIN/pillar/... or on the node /srv/pillar )
If using the formula standalone, the data needs to be provided manually. There are more options available as shown in the example file.

*Example HANA pillar*
[subs="attributes,quotes"]
----
hana:
  saptune_solution: 'HANA'
  nodes:
    - host: '_hana01_'
      sid: '_prd_'
      instance: "_00_"
      password: '_SET YOUR PASSWORD_'
      install:
        software_path: '/sapmedia/HANA'
        root_user: 'root'
        root_password: ''
        system_user_password: '_SET YOUR PASSWORD_'
        sapadm_password: '_SET YOUR PASSWORD_'
      primary:
        name: _PRIMARY_SITE_NAME_
        backup:
          key_name: 'backupkey'
          database: 'SYSTEMDB'
          file: 'backup'
        userkey:
          key_name: 'backupkey'
          environment: '_hana01_:30013'
          user_name: 'SYSTEM'
          user_password: '_SET YOUR PASSWORD_'
          database: 'SYSTEMDB'

    - host: '_hana02_'
      sid: '_prd_'
      instance: "_00_"
      password: '_SET YOUR PASSWORD_'
      install:
        software_path: '/sapmedia/HANA'
        root_user: 'root'
        root_password: ''
        system_user_password: '_SET YOUR PASSWORD_'
        sapadm_password: '_SET YOUR PASSWORD_'
      secondary:
        name: _SECONDARY_SITE_NAME_
        remote_host: '_hana01_'
        remote_instance: "_00_"
        replication_mode: 'sync'
        operation_mode: 'logreplay'
        primary_timeout: 3000
----

. The formula is executed within the _hana_node_ Salt state files.

. If wanting to execute the formula manually
+
----
salt '*' state.apply hana_node.sls
----

With the help of the pillar data, the state file, and the formula, Salt will create all needed configuration on the node, will install HANA and, if enabled, will install hana systemreplication and set up the pacemaker cluster, correctly for {cloud}.

The _templates_ directory provides the needed files for cluster rules, the needed hook for HANA, and the monitoring exporter.  All the values come from the best practices guides SUSE created with the Cloud provider {cloud} for the HA scenario.


===== Netweaver formula

The SAP Netweaver deployment is performed using the _sapnwbootstrap-formula_ and uses, as of today, only SAP HANA as a database.

The formula takes care of the ASCS, the Application Servers, and, if HA is selected, the Enqueue Replication server.

The formula has some *hard dependencies* and *all of them must be in place* for a successful netweaver deployment. In order to deploy a valid Netweaver environment, a NFS share is needed (SAP stores shared files there). The NFS share must have the folders _sapmnt_ and _usrsapsys_ in the exposed folder.
The folders are created with the Netweaver SID name (e.g., /sapdata/HA1/sapmnt and /sapdata/HA1/usrsapsys). This content is removed by default during the deployment.

Secondly, the SAP installation software (SWPM) must be available in the system.
To install the whole Netweaver environment with all the 4 components, the SAP Media must be provided. The structure depends on the version of SWPM.

//FIXME - SWPM 1+2 and s4 example should be provided in the Appendix
For SWPM 1.0 the swpm folder, sapexe folder, Netweaver Export folder and HANA HDB Client folders must already exist, or be previously mounted when provided by external service, such as NFS share. The netweaver.sls pillar file must be updated with all this information. Netweaver Export and HANA HDB Client folders must go in additional_dvds list.

The structure is similar what has been illustrated above for the HANA formula.

[subs="attributes,quotes"]
----
states/
└── ...
└── netweaver
    ├── defaults.yaml
    ├── ensa_version_detection.sls
    ├── extract_nw_archives.sls
    ├── ha_cluster.sls
    ├── *init.sls*
    ├── install_aas.sls
    ├── install_ascs.sls
    ├── install_db.sls
    ├── install_ers.sls
    ├── install_pas.sls
    ├── install_pydbapi.sls
    ├── map.jinja
    ├── monitoring.sls
    ├── pillar.example
    ├── pre_validation.sls
    ├── saptune.sls
    ├── setup
    │   ├── init.sls
    │   ├── keepalive.sls
    │   ├── mount.sls
    │   ├── packages.sls
    │   ├── sap_nfs.sls
    │   ├── shared_disk.sls
    │   ├── swap_space.sls
    │   ├── users.sls
    │   └── virtual_addresses.sls
    └── templates
        ├── aas.inifile.params.j2
        ├── ascs.inifile.params.j2
        ├── cluster_resources.j2
        ├── db.inifile.params.j2
        ├── ers.inifile.params.j2
        └── pas.inifile.params.j2
----

As described earlier, a pillar file is needed with the configuration. There is one example in the path, which can be used as a base for standalone Salt usage. In general, the pillar data will be passed from the Terraform main project.

As SAP Netweaver has additional nodes in an HA environment, the pillar file will be larger than the one for HANA. Take the time to review this by viewing the example file.

Similar to before, the starting point is the _init.sls_ file, where the workflow is defined.

The _templates_ directory provides the needed files for Netweaver cluster rules, and the values come from the best practices guides SUSE created with {cloud} for the ERS scenario.

In addition, here are the templates which are used by SWPM for an automated hands-free installation of the SAP Netweaver services.

==== High Availability formula

The _habootstrap-formula_ will take care of the needed cluster setup for SAP HANA, SAP Netweaver, and, if needed, for the HA NFS service built with DRBD.

The formula will be similar to all the other formulas used and installed in /usr/share/salt-formulas/states/cluster.

[subs="attributes,quotes"]
----
states
├── cluster
│   ├── create.sls
│   ├── defaults.yaml
│   ├── *init.sls*
│   ├── join.sls
│   ├── map.jinja
│   ├── monitoring.sls
│   ├── ntp.sls
│   ├── packages.sls
│   ├── pre_validation.sls
│   ├── remove.sls
│   ├── resource_agents.sls
│   ├── sshkeys.sls
│   ├── support
│   │   └── ssh_askpass
│   └── watchdog.sl
----

The main difference to the HANA and Netweaver formula is that the _init.sls_ already makes use of _jinja_.
Jinja is the default templating language in SLS files and gets evaluated before YAML, which means it is evaluated before the states are run.

The most basic usage of Jinja in state files is using control structures to wrap conditional or redundant state elements.


==== Additional Services

The additional services depend on what is used or available from the cloud provider, but needed by SAP HANA or SAP Netweaver or the HA services.

ifeval::[ "{cloud}" == "Azure" ]

===== NFS service

To build an HA-NFS service, we use the above described _habootstrap-formula_ together with _drbd-formula_ to mirror the data between two nodes and the _linux nfs-server: packages been setup with the SaltStack _nfs_formula ( see https://github.com/saltstack-formulas/nfs-formula )

DRBD®– software is a distributed replicated storage system for the Linux platform. It is implemented as a kernel driver, several userspace management applications, and some shell scripts. So think about it as "RAID-1 over network."

Details are available at the SUSE documentation page for the SLE HA Extension
https://documentation.suse.com/sle-ha/15-SP2/single-html/SLE-HA-nfs-quick/#art-sleha-nfs-quick

===== Fencing service

If the setup is using HA for SAP Netweaver or for SAP HANA or with the NFS service, and there is no mechanism for fencing of the virtual machines over an API, we use the SUSE SBD-device method. Such a SBD-Device is normally a raw shared disk beween two nodes.

Unfortunately not all clouds are able to provide a raw shared disk, but with the help of Linux native services (iSCSI) we can build this by our own.

We use here the _iscsi-formula_ provided by SaltStack itself (see https://github.com/saltstack-formulas/iscsi-formula) to provide to the nodes of the cluster a raw-shared-disk with help of a _iscsi target_ for the SBD fencing mechanism.

It gets configured through the pillar files we provided through the role _iscsi_srv_

The use of possible fencing method depends on the cloud provider's features. As of today, SBD is needed only for Azure, but it is a general method which could be used nearly independent of the base infrastructure.
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
 
===== NFS service

In order to use NFS, AWS EFS Service is recommended.  No additional SUSE infrastructure needs to be deployed.

==== Fencing service

Fencing in the cluster will be provided by the EC2 Fencing capability.  This mechanism uses the AWS API framework to fence nodes as required.  The additional IAM and AWS requirements are setup as part of the automation workflow.  No additional SUSE Infrastructure needs to be deployed.

endif::[]

ifeval::[ "{cloud}" == "GCP" ]
 GCP
endif::[]

ifeval::[ "{cloud}" == "Libvirt" ]
 Libvirt
endif::[]

// fixme - add monitoring
//===== Monitoring service
//golang-github-prometheus-node_exporter
//prometheus-ha_cluster_exporter
//prometheus-hanadb_exporter
//prometheus-sap_host_exporter
